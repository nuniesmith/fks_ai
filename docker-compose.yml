
services:
  # Ollama for local LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: fks-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    restart: unless-stopped
    networks:
      - fks-network
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  fks_ai:
    build:
      context: .
      dockerfile: docker/Dockerfile
    image: nuniesmith/fks:ai-latest
    container_name: fks-ai
    ports:
      - "8007:8007"
    environment:
      - SERVICE_NAME=fks_ai
      - SERVICE_PORT=8007
      - PYTHONPATH=/app/src:/app
      # Google AI API (Gemini) - Free Tier Configuration
      - GOOGLE_AI_API_KEY=${GOOGLE_AI_API_KEY:-}
      - GEMINI_LLM_MODEL=${GEMINI_LLM_MODEL:-gemini-2.0-flash-exp}
      - GEMINI_EMBEDDING_MODEL=${GEMINI_EMBEDDING_MODEL:-models/embedding-001}
      - GEMINI_FREE_TIER_LIMIT=${GEMINI_FREE_TIER_LIMIT:-1500}  # Daily free tier limit (1,500-10,000 prompts/day)
      # Ollama (Fallback/Local LLM)
      - OLLAMA_HOST=http://ollama:11434
      - LLM_PROVIDER=${LLM_PROVIDER:-gemini}  # Use 'gemini' for Google AI API, 'ollama' for local
      - LLM_MODEL=${LLM_MODEL:-gemini-2.0-flash-exp}  # Default to Gemini, fallback to ollama if API key not set
    networks:
      - fks-network
    # depends_on:
    #   ollama:
    #     condition: service_healthy
    # Note: Dependency commented out - Ollama may take time to become healthy
    # fks_ai will gracefully degrade if Ollama is not available
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8007/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

volumes:
  ollama_models:
    driver: local

networks:
  fks-network:
    external: true
