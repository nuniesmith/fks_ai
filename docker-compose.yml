services:
  # Ollama for local LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: fks-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    restart: unless-stopped
    networks:
      - fks-network
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  fks_ai:
    build:
      context: .
      dockerfile: docker/Dockerfile
    image: nuniesmith/fks:ai-latest
    container_name: fks-ai
    ports:
      - "8007:8007"
    environment:
      - SERVICE_NAME=fks_ai
      - SERVICE_PORT=8007
      - PYTHONPATH=/app/src:/app

      # LiteLLM Model Selection (one env var to rule them all)
      # Options: ollama/llama3.2:3b, groq/llama-3.3-70b-versatile, gemini/gemini-1.5-flash, etc.
      - LITELLM_MODEL=${LITELLM_MODEL:-ollama/llama3.2:3b}

      # LLM Configuration
      - LLM_TEMPERATURE=${LLM_TEMPERATURE:-0.1}
      - LLM_MAX_TOKENS=${LLM_MAX_TOKENS:-2048}
      - LLM_TIMEOUT=${LLM_TIMEOUT:-120}

      # Ollama (Local LLM - always available for development)
      - OLLAMA_HOST=http://ollama:11434

      # Cloud Provider API Keys (optional, for switching to cloud models)
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-} # For gemini/* models (FREE)
      - GROQ_API_KEY=${GROQ_API_KEY:-} # For groq/* models
      - CEREBRAS_API_KEY=${CEREBRAS_API_KEY:-} # For cerebras/* models
      - OPENAI_API_KEY=${OPENAI_API_KEY:-} # For openai/* models
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-} # For anthropic/* models

      # Legacy Google AI API settings (deprecated, use LITELLM_MODEL instead)
      - GOOGLE_AI_API_KEY=${GOOGLE_AI_API_KEY:-}
      - GEMINI_LLM_MODEL=${GEMINI_LLM_MODEL:-gemini-2.0-flash-exp}
      - GEMINI_EMBEDDING_MODEL=${GEMINI_EMBEDDING_MODEL:-models/embedding-001}
      - GEMINI_FREE_TIER_LIMIT=${GEMINI_FREE_TIER_LIMIT:-1500}
      - LLM_PROVIDER=${LLM_PROVIDER:-gemini}
      - LLM_MODEL=${LLM_MODEL:-gemini-2.0-flash-exp}
    networks:
      - fks-network
    # depends_on:
    #   ollama:
    #     condition: service_healthy
    # Note: Dependency commented out - Ollama may take time to become healthy
    # fks_ai will gracefully degrade if Ollama is not available
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8007/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

volumes:
  ollama_models:
    driver: local

networks:
  fks-network:
    external: true
